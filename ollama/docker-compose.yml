services:
  ollama:
    container_name: ollama
    image: ollama/ollama
    hostname: docker-ollama
    expose:
      - 11434/tcp
    # NVIDIA GPU
    # https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html
    # environment:
    #   - NVIDIA_VISIBLE_DEVICES=all
    #   - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - capabilities: [gpu]
    # runtime: nvidia

    # AMD GPU
    # https://rocm.docs.amd.com/en/latest/deploy/docker.html
    # devices:
    #   - /dev/kfd:/dev/kfd
    #   - /dev/dri:/dev/dri
    # environment:
    #   - HSA_OVERRIDE_GFX_VERSION=10.3.0
    volumes:
      - ollama:/root/.ollama
    networks:
      - llm-public
    restart: always

  open-webui:
    container_name: open-webui
    image: ghcr.io/open-webui/open-webui:main
    hostname: docker-open-webui
    ports:
      - 3000:8080/tcp
    volumes:
      - open-webui:/app/backend/data
    environment:
      OLLAMA_BASE_URL: http://ollama:11434
    depends_on:
      - ollama
    networks:
      - llm-public
    restart: always

volumes:
  ollama: {}
  open-webui: {}

networks:
  llm-public:
    external: true
